{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/food-classification-using-tensorflow-1716602f-48aa-41db-b4e1-835cb05b4ec3.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240306/auto/storage/goog4_request&X-Goog-Date=20240306T194338Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=b46b9a8b1fb947ff73d7f9837db005eb33a904581f36393eaa3787987542d1063904d1b3f63e630fd0c323cd0c40b746fe7827e7b338ad29978ee6cce402b9b8c23a70e6b54cc6704ea217641f8fcc3d6b9625fbb18be803d6f7ca9f4264f8e704ec320b26fe6c687afa44616e73f390fa76b07a6af69556d52e234cae8e836d4ebf2315d22fbf992859a05ef090630e12ecb450da96774b1715f85e6f3616fc956882c988572b201f189aeb7a1276a497affe6246d5b51fbd56009aaa6bfc57c947aa69aec85edef96d18e5a4dc8ba5e257fb47d6d0104b8a306569f5a2a6072af80cf7829ac671cf37c871350127b136957c9771672b942dc65970a650f1a4","timestamp":1709754290862}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"source":["\n","import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'indian-food-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1374528%2F2287227%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240306%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240306T194338Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D22cdffc67266fb000e0186c1a9f888e13c0f7dc9d0b089662ab77f73a6124bb26234bb7fb783e83e0dfa97ea9fd08cc1212953f85f4d70616fca3782de6b03a33826628dd5a86fed2217957cfe02b1a22069e2427aef0a40d0ab1042d918992afff4bfa5e5299c7d08daccf085b36bd72c477e58dbb4a897c9676f40cc54db0e011a754d7247336429569ee159f600f277c18c8577dc02b5b178d0b7bdd8124177fad0dbda1505b02047b09002eeaef9e94b240e0aa788ed317eae24dd5230af8a8da262955140a45b00adbbee711bd8999e050f3f108a5a2ae639440e9edf406f12c95792f8393582a2fc936122ccb957d012ed05aa47c33baf01afdb52cc76'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')\n"],"metadata":{"id":"9eLlm3UbSwcV"},"cell_type":"code","outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# <center>Food Classification using TensorFlow (CNN Transfer learning)</center>\n","<center><img src= \"https://timesofindia.indiatimes.com/thumb/msid-77563051,width-1200,height-900,resizemode-4/.jpg\" alt =\"Titanic\" style='width:500px;'></center><br>"],"metadata":{"execution":{"iopub.status.busy":"2021-05-30T14:59:50.125178Z","iopub.execute_input":"2021-05-30T14:59:50.125937Z","iopub.status.idle":"2021-05-30T14:59:50.131972Z","shell.execute_reply.started":"2021-05-30T14:59:50.125879Z","shell.execute_reply":"2021-05-30T14:59:50.130719Z"},"id":"qdwIESnbSwcd"}},{"cell_type":"markdown","source":["- <h3> Image classification is the task of identifying images and categorizing them in one of several predefined distinct classes.</h3><br>\n","- <h3> Being one of the computer vision (CV) tasks, Image classification serves as the foundation for solving different CV problems such as object detection which further gets divided into semantic and instance segmentation.</h3><br>\n","- <h3> The leading architecture used for image recognition and detection tasks is Convolutional Neural Networks (CNNs). Convolutional neural networks consist of several layers with small neuron collections, each of them perceiving small parts of an image. </h3>"],"metadata":{"id":"me1ZyGXjSwcf"}},{"cell_type":"code","source":["import os\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T12:43:16.144006Z","iopub.execute_input":"2021-06-02T12:43:16.144526Z","iopub.status.idle":"2021-06-02T12:43:16.150927Z","shell.execute_reply.started":"2021-06-02T12:43:16.144464Z","shell.execute_reply":"2021-06-02T12:43:16.149899Z"},"trusted":true,"id":"W4JwUxjTSwcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras import models\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras import models\n","from PIL import Image\n","from skimage.io import imread\n","import cv2\n","\n","K.clear_session()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:43:16.867405Z","iopub.execute_input":"2021-06-02T12:43:16.868085Z","iopub.status.idle":"2021-06-02T12:43:16.875196Z","shell.execute_reply.started":"2021-06-02T12:43:16.868049Z","shell.execute_reply":"2021-06-02T12:43:16.874489Z"},"trusted":true,"id":"B8Eg9mGmSwcg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2> <span style=\"color:red\"><b> CHALLENGES </b></span></h2><hr> <h3>1. Cleaning the dataset due to following :</h3>\n","    \n","- A lot of misspelled labels.<br>\n","- Overwhelming number of raw images not related to any class.<br>\n","\n","<h3> 2. Training model without bias : </h3>\n","\n","- Similarly looking items with common attributes (Fried samosa and pakode looks familiar.)<br>\n","- Multiple labels clashing together (Butter Naan and Dal Makhni in one picture, Samosa and Pakode, etc.)<br>\n","- Unwanted feature selection because of big architectures in transfer learning (Ketchup and green chutney is most common thing alongside pakode, samosa, kathi rolls and what not.)<br>\n","\n","<hr>\n","\n","<h2> <span style=\"color:green\"> Finally after 4-5 hours of data cleaning and correcting labels, it was time for fun! </span> </h2>"],"metadata":{"execution":{"iopub.status.busy":"2021-05-30T07:54:24.4964Z","iopub.execute_input":"2021-05-30T07:54:24.49674Z","iopub.status.idle":"2021-05-30T07:54:24.503093Z","shell.execute_reply.started":"2021-05-30T07:54:24.496707Z","shell.execute_reply":"2021-05-30T07:54:24.50208Z"},"id":"g955yVBySwch"}},{"cell_type":"markdown","source":["# <b>IMAGE PROCESSSING</b>"],"metadata":{"id":"ZNsSGSbDSwci"}},{"cell_type":"markdown","source":["<h2>Reshaping dimensions so we can start processing arrays. </h2>"],"metadata":{"execution":{"iopub.status.busy":"2021-05-31T17:24:49.652698Z","iopub.execute_input":"2021-05-31T17:24:49.653067Z","iopub.status.idle":"2021-05-31T17:24:49.658388Z","shell.execute_reply.started":"2021-05-31T17:24:49.653038Z","shell.execute_reply":"2021-05-31T17:24:49.657247Z"},"id":"oPogbfweSwcj"}},{"cell_type":"code","source":["img = plt.imread('../input/indian-food-classification/dataset/Dataset/train/pizza/033.jpg')\n","dims = np.shape(img)\n","matrix = np.reshape(img, (dims[0] * dims[1], dims[2]))\n","print(np.shape(matrix))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:43:18.879755Z","iopub.execute_input":"2021-06-02T12:43:18.880282Z","iopub.status.idle":"2021-06-02T12:43:18.903211Z","shell.execute_reply.started":"2021-06-02T12:43:18.880249Z","shell.execute_reply":"2021-06-02T12:43:18.90237Z"},"trusted":true,"id":"pdCpAzv8Swcj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(img)\n","print(\"Image shape -> \",dims[:2])\n","print(\"Color channels -> \",dims[2])\n","print(\"Min color depth : {}, Max color depth {}\".format(np.min(img),np.max(img)))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:43:19.128829Z","iopub.execute_input":"2021-06-02T12:43:19.129375Z","iopub.status.idle":"2021-06-02T12:43:19.382965Z","shell.execute_reply.started":"2021-06-02T12:43:19.129341Z","shell.execute_reply":"2021-06-02T12:43:19.381814Z"},"trusted":true,"id":"omlAuvn_Swck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2>Plot for visualizing pixel intensities for RGB in color space</h2>"],"metadata":{"id":"3IzBF1gVSwck"}},{"cell_type":"code","source":["sns.distplot(matrix[:,0], bins=20,color=\"red\",hist_kws=dict(alpha=0.3))\n","sns.distplot(matrix[:,1], bins=20,color=\"green\",hist_kws=dict(alpha=0.35))\n","sns.distplot(matrix[:,2], bins=20,color=\"blue\",hist_kws=dict(alpha=0.2))\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:43:19.519211Z","iopub.execute_input":"2021-06-02T12:43:19.519666Z","iopub.status.idle":"2021-06-02T12:43:30.168874Z","shell.execute_reply.started":"2021-06-02T12:43:19.519632Z","shell.execute_reply":"2021-06-02T12:43:30.167639Z"},"trusted":true,"id":"KQz8hjqcSwck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2>Plot for visualizing histogram between 2 color channel</h2>"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T14:19:20.241654Z","iopub.execute_input":"2021-06-01T14:19:20.242148Z","iopub.status.idle":"2021-06-01T14:19:20.249804Z","shell.execute_reply.started":"2021-06-01T14:19:20.242106Z","shell.execute_reply":"2021-06-01T14:19:20.247872Z"},"id":"yU53KGfMSwcl"}},{"cell_type":"code","source":["_ = plt.hist2d(matrix[:,1], matrix[:,2], bins=(50,50))\n","plt.xlabel('Green channel')\n","plt.ylabel('Blue channel')\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:43:30.170414Z","iopub.execute_input":"2021-06-02T12:43:30.170832Z","iopub.status.idle":"2021-06-02T12:43:30.354835Z","shell.execute_reply.started":"2021-06-02T12:43:30.170796Z","shell.execute_reply":"2021-06-02T12:43:30.353906Z"},"trusted":true,"id":"vUFxXbs4Swcl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- <span style=\"font-size:18px;color:blue\"><b>The pixels between green and blue bands are correlated (as evident from overlapping on above graph), and typically has visible imagery.</b></span>\n","\n","- <span style=\"font-size:18px;color:blue\"><b>Raw band differences will need to be scaled or thresholded </b></span>"],"metadata":{"execution":{"iopub.status.busy":"2021-05-31T17:35:14.516458Z","iopub.execute_input":"2021-05-31T17:35:14.516828Z","iopub.status.idle":"2021-05-31T17:35:14.523085Z","shell.execute_reply.started":"2021-05-31T17:35:14.5168Z","shell.execute_reply":"2021-05-31T17:35:14.521731Z"},"id":"rS9zYTBpSwcl"}},{"cell_type":"markdown","source":["<span style=\"font-size:15px;\"><b>Image data consists of variations due to resolution differences between scenes, pixel intensities of an image and the environment around which the image was taken. This area of image processing is critical in today's time with the rise of Artificial intelligence.From motion detection to complex circuits in self driving car, the research requires tremendous amount of work and can be seen as widely growing areas of computer vision.</b><span>"],"metadata":{"id":"edHxg6qUSwcl"}},{"cell_type":"code","source":["from sklearn import cluster\n","n_vals=[2,4,6,8]\n","plt.figure(1, figsize=(12, 8))\n","\n","for subplot,n in enumerate(n_vals):\n","    kmeans=cluster.KMeans(n)\n","    clustered = kmeans.fit_predict(matrix)\n","    dims = np.shape(img)\n","    clustered_img = np.reshape(clustered, (dims[0], dims[1]))\n","    plt.subplot(2,2, subplot+1)\n","    plt.title(\"n = {}\".format(n), pad = 10,size=18)\n","    plt.imshow(clustered_img)\n","\n","plt.tight_layout()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:43:30.35656Z","iopub.execute_input":"2021-06-02T12:43:30.356839Z","iopub.status.idle":"2021-06-02T12:43:58.610807Z","shell.execute_reply.started":"2021-06-02T12:43:30.356811Z","shell.execute_reply":"2021-06-02T12:43:58.609749Z"},"trusted":true,"id":"Hu6ozcOISwcl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2> Let's visualize the channel intensity for every cluster we just generated.</h2>"],"metadata":{"id":"yy5609zxSwcm"}},{"cell_type":"code","source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","fig=plt.figure(figsize=(14,10))\n","\n","ax = [fig.add_subplot(221, projection='3d'),\n","      fig.add_subplot(222, projection='3d'),\n","      fig.add_subplot(223, projection='3d'),\n","      fig.add_subplot(224, projection='3d')]\n","\n","for plot_number,n in enumerate(n_vals):\n","\n","    kmeans=cluster.KMeans(n)\n","    clustered = kmeans.fit_predict(matrix)\n","    x1, y1, z1 = [np.where(clustered == x)[0] for x in [0, 1, 2]]\n","\n","    plot_vals = [('r', x1),\n","                 ('b', y1),\n","                 ('g', z1),\n","                 ]\n","\n","    for c, channel in plot_vals:\n","        x = matrix[channel, 0]\n","        y = matrix[channel, 1]\n","        z = matrix[channel, 2]\n","        ax[plot_number].scatter(x, y, z, c=c,s=10)\n","\n","    ax[plot_number].set_xlabel('Blue channel')\n","    ax[plot_number].set_ylabel('Green channel')\n","    ax[plot_number].set_zlabel('Red channel')\n","\n","plt.tight_layout()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:43:58.612395Z","iopub.execute_input":"2021-06-02T12:43:58.612724Z","iopub.status.idle":"2021-06-02T12:44:53.711055Z","shell.execute_reply.started":"2021-06-02T12:43:58.612693Z","shell.execute_reply":"2021-06-02T12:44:53.709978Z"},"trusted":true,"id":"sCcZyPwNSwcm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<center><span style=\"font-size:18px;color:blue\"><b>It is evident from above graph, intensity is reduced in the color space as the number of clusters are increased.</b></span></center>"],"metadata":{"id":"mHfi1abmSwcm"}},{"cell_type":"markdown","source":["<h2> Brightness normalization is a process that changes the range of pixel intensity values. Applications include photographs with poor contrast due to glare, for example. Normalization is sometimes called contrast stretching or histogram stretching. </h2>"],"metadata":{"id":"24bpnX1hSwcm"}},{"cell_type":"code","source":["bnorm = np.zeros_like(matrix, dtype=np.float32)\n","max_range = np.max(matrix, axis=1)\n","bnorm = matrix / np.vstack((max_range, max_range, max_range)).T\n","bnorm_img = np.reshape(bnorm, (dims[0],dims[1],dims[2]))\n","plt.figure(figsize=(8,10))\n","plt.imshow(bnorm_img)\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:44:53.712581Z","iopub.execute_input":"2021-06-02T12:44:53.71315Z","iopub.status.idle":"2021-06-02T12:44:54.093524Z","shell.execute_reply.started":"2021-06-02T12:44:53.713106Z","shell.execute_reply":"2021-06-02T12:44:54.091878Z"},"trusted":true,"id":"nfllbn7eSwcm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" <h2> Sobel filter is a basic way to get an edge magnitude/gradient image. </h2>\n","    \n","**It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction.**"],"metadata":{"id":"Qn5p2psBSwcn"}},{"cell_type":"code","source":["import skimage\n","from skimage.feature import greycomatrix, greycoprops\n","from skimage.filters import sobel\n","from skimage.filters import sobel_h\n","\n","plt.figure(1,figsize=(20,15))\n","cmap=\"YlGnBu\"\n","plt.subplot(3,1,1)\n","plt.imshow(img)\n","\n","plt.subplot(3,1,2)\n","plt.imshow(sobel(img[:,:,2]),cmap=cmap)\n","\n","plt.subplot(3,1,3)\n","plt.imshow(sobel_h(img[:,:,1]), cmap=cmap)\n","\n","plt.tight_layout()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:44:54.096242Z","iopub.execute_input":"2021-06-02T12:44:54.096986Z","iopub.status.idle":"2021-06-02T12:44:55.124757Z","shell.execute_reply.started":"2021-06-02T12:44:54.096935Z","shell.execute_reply":"2021-06-02T12:44:55.123653Z"},"trusted":true,"id":"r3oRK3bbSwcn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<center><span style=\"font-size:18px;color:blue\"><b>Clearly the results are fascinating. We are able to isolate object and detect the edge. </b></span></center>"],"metadata":{"id":"wpmnEmfQSwcn"}},{"cell_type":"markdown","source":["<h2> Let's apply Principal Component Analysis. It is a linear dimensionality reduction technique that can be utilized for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space.</h2>"],"metadata":{"id":"Ov3LQae_Swcn"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(3)\n","pca.fit(matrix)\n","img_pca = pca.transform(matrix)\n","img_pca = np.reshape(img_pca, (dims[0], dims[1], dims[2]))\n","\n","fig = plt.figure(figsize=(8, 8))\n","plt.imshow(img_pca[:,:,1], cmap=cmap)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:44:55.126263Z","iopub.execute_input":"2021-06-02T12:44:55.12686Z","iopub.status.idle":"2021-06-02T12:44:55.573752Z","shell.execute_reply.started":"2021-06-02T12:44:55.126823Z","shell.execute_reply":"2021-06-02T12:44:55.572623Z"},"trusted":true,"id":"4eL6WjP5Swcn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <b>Exploratory Data Analysis (EDA)</b>"],"metadata":{"id":"RO5253ugSwco"}},{"cell_type":"markdown","source":["## Let's visualize number of training examples for each food item"],"metadata":{"id":"p0MAdHIjSwco"}},{"cell_type":"code","source":["main='../input/indian-food-classification/dataset/Dataset/train/'\n","\n","data=dict()\n","\n","for i in os.listdir(main):\n","    sub_dir=os.path.join(main,i)\n","    count=len(os.listdir(sub_dir))\n","    data[i]=count\n","\n","\n","keys = data.keys()\n","values = data.values()\n","\n","colors=[\"red\" if x<= 150 else \"green\" for x in values]\n","\n","fig, ax = plt.subplots(figsize=(12,8))\n","y_pos=np.arange(len(values))\n","plt.barh(y_pos,values,align='center',color=colors)\n","for i, v in enumerate(values):\n","    ax.text(v+1.4, i-0.25, str(v), color=colors[i])\n","ax.set_yticks(y_pos)\n","ax.set_yticklabels(keys)\n","ax.set_xlabel('Images',fontsize=16)\n","plt.xticks(color='black',fontsize=13)\n","plt.yticks(fontsize=13)\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:44:55.576504Z","iopub.execute_input":"2021-06-02T12:44:55.577127Z","iopub.status.idle":"2021-06-02T12:44:55.89176Z","shell.execute_reply.started":"2021-06-02T12:44:55.577076Z","shell.execute_reply":"2021-06-02T12:44:55.890295Z"},"trusted":true,"id":"OmE3IvhSSwco"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h3>We notice that 2 classes (Pani puri and Kulfi) lacks behind with training data.</h3>\n","\n","> - <span style=\"font-size:18px;color:blue\">Data augmentation helps with classes not having enough training examples by increasing the amount of relevant data in the dataset.</span>\n","<br><br>\n","> - <span style=\"font-size:18px;color:blue\"> We would be doing what is known as **offline augmentation**. It works on relatively smaller datasets, by increasing the size of the dataset by a factor equal to the number of transformations you perform. (For example, by flipping all my images, it would increase the size of the dataset by a factor of 2).</span>\n","<br><br>\n","> - <span style=\"font-size:18px;color:blue\"><u>You will get more clarity in the coming section</u></span>"],"metadata":{"id":"C1uBJv5JSwco"}},{"cell_type":"markdown","source":["## Let's visualize our dataset by randomly picking an image from every class"],"metadata":{"execution":{"iopub.status.busy":"2021-05-30T07:55:30.495891Z","iopub.execute_input":"2021-05-30T07:55:30.496241Z","iopub.status.idle":"2021-05-30T07:55:30.500117Z","shell.execute_reply.started":"2021-05-30T07:55:30.496187Z","shell.execute_reply":"2021-05-30T07:55:30.499297Z"},"id":"8t0zyZzMSwco"}},{"cell_type":"code","source":["import random\n","\n","train_folder = \"/kaggle/input/indian-food-classification/dataset/Dataset/train\"\n","images = []\n","\n","for food_folder in sorted(os.listdir(train_folder)):\n","    food_items = os.listdir(train_folder + '/' + food_folder)\n","    food_selected = np.random.choice(food_items)\n","    images.append(os.path.join(train_folder,food_folder,food_selected))\n","\n","fig=plt.figure(1, figsize=(25, 25))\n","\n","for subplot,image_ in enumerate(images):\n","    category=image_.split('/')[-2]\n","    imgs = plt.imread(image_)\n","    a,b,c=imgs.shape\n","    fig=plt.subplot(5, 4, subplot+1)\n","    fig.set_title(category, pad = 10,size=18)\n","    plt.imshow(imgs)\n","\n","plt.tight_layout()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:44:55.89377Z","iopub.execute_input":"2021-06-02T12:44:55.894231Z","iopub.status.idle":"2021-06-02T12:45:04.297743Z","shell.execute_reply.started":"2021-06-02T12:44:55.894184Z","shell.execute_reply":"2021-06-02T12:45:04.296654Z"},"trusted":true,"id":"ULFi0bcHSwcp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<hr>"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T14:28:24.08862Z","iopub.execute_input":"2021-06-01T14:28:24.089057Z","iopub.status.idle":"2021-06-01T14:28:24.094632Z","shell.execute_reply.started":"2021-06-01T14:28:24.089006Z","shell.execute_reply":"2021-06-01T14:28:24.093189Z"},"id":"s1-B9nipSwcp"}},{"cell_type":"markdown","source":["## We discussed Data Augmentation before. Let's see how it works:\n","<span style=\"font-size:16px\"> 1. Accepting a batch of images used for training.</span>\n","\n","<span style=\"font-size:16px\"> 2. Taking this batch and applying a series of random transformations to each image in the batch. (including random rotation, resizing, shearing, etc.)</span>\n","\n","<span style=\"font-size:16px\"> 3. Replacing the original batch with the new, randomly transformed batch.</span>\n","\n","<span style=\"font-size:16px\"> 4. Training the CNN on this randomly transformed batch. (i.e, the original data itself is not used for training)</span>\n","\n","<center><img src=\"https://miro.medium.com/max/1700/1*ae1tW5ngf1zhPRyh7aaM1Q.png\" width=500></center>"],"metadata":{"id":"vUzuRRaHSwcp"}},{"cell_type":"markdown","source":["<hr>"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T14:28:45.455707Z","iopub.execute_input":"2021-06-01T14:28:45.456093Z","iopub.status.idle":"2021-06-01T14:28:45.463292Z","shell.execute_reply.started":"2021-06-01T14:28:45.456034Z","shell.execute_reply":"2021-06-01T14:28:45.461661Z"},"id":"NaqBe1mASwcp"}},{"cell_type":"markdown","source":["# <b> MODEL TRAINING </b>"],"metadata":{"id":"Dc36NTN3Swcp"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","n_classes = 20\n","batch_size = 32\n","img_width, img_height = 299, 299\n","\n","train_data_dir = '/kaggle/input/indian-food-classification/dataset/Dataset/train'\n","\n","# Data Augmentation with ImageDataGenerator\n","train_datagen = ImageDataGenerator(\n","    rescale=1. / 255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical')\n","\n","val_data_dir = '/kaggle/input/indian-food-classification/dataset/Dataset/val'\n","\n","val_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","val_generator = val_datagen.flow_from_directory(\n","    val_data_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:45:04.299165Z","iopub.execute_input":"2021-06-02T12:45:04.299619Z","iopub.status.idle":"2021-06-02T12:45:04.642534Z","shell.execute_reply.started":"2021-06-02T12:45:04.299571Z","shell.execute_reply":"2021-06-02T12:45:04.641651Z"},"trusted":true,"id":"7BZJDQ-ZSwcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_map = train_generator.class_indices\n","class_map"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:45:04.643568Z","iopub.execute_input":"2021-06-02T12:45:04.64397Z","iopub.status.idle":"2021-06-02T12:45:04.649577Z","shell.execute_reply.started":"2021-06-02T12:45:04.64394Z","shell.execute_reply":"2021-06-02T12:45:04.648623Z"},"trusted":true,"id":"y9A542QaSwcq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(tf.__version__)\n","# print(tf.test.gpu_device_name())"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:45:04.650944Z","iopub.execute_input":"2021-06-02T12:45:04.651239Z","iopub.status.idle":"2021-06-02T12:45:04.663149Z","shell.execute_reply.started":"2021-06-02T12:45:04.651211Z","shell.execute_reply":"2021-06-02T12:45:04.662108Z"},"trusted":true,"id":"GdsUYDGUSwcv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training the model"],"metadata":{"execution":{"iopub.status.busy":"2021-05-30T08:02:58.333074Z","iopub.execute_input":"2021-05-30T08:02:58.333421Z","iopub.status.idle":"2021-05-30T08:02:58.337311Z","shell.execute_reply.started":"2021-05-30T08:02:58.333394Z","shell.execute_reply":"2021-05-30T08:02:58.336226Z"},"id":"5lBNmXsPSwcv"}},{"cell_type":"code","source":["from tensorflow.keras.applications.inception_v3 import InceptionV3\n","# from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n","# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.layers import Dense, Dropout\n","\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.regularizers import l2\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","\n","nb_train_samples = 3583\n","nb_validation_samples = 1089\n","\n","inception = InceptionV3(weights='imagenet', include_top=False)\n","x = inception.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(128,activation='relu')(x)\n","x = Dropout(0.2)(x)\n","\n","predictions = Dense(n_classes,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n","\n","model = Model(inputs=inception.input, outputs=predictions)\n","model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n","checkpointer = ModelCheckpoint(filepath='v1_inceptionV3', verbose=1, save_best_only=True)\n","csv_logger = CSVLogger('history_v1_inceptionV3.log')\n","\n","history = model.fit_generator(train_generator,\n","                    steps_per_epoch = nb_train_samples // batch_size,\n","                    validation_data=val_generator,\n","                    validation_steps=nb_validation_samples // batch_size,\n","                    epochs=20,\n","                    verbose=1,\n","                    callbacks=[csv_logger, checkpointer])"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:45:04.664605Z","iopub.execute_input":"2021-06-02T12:45:04.664926Z","iopub.status.idle":"2021-06-02T12:45:18.670771Z","shell.execute_reply.started":"2021-06-02T12:45:04.664895Z","shell.execute_reply":"2021-06-02T12:45:18.666934Z"},"trusted":true,"id":"JRI3yZ0zSwcw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving the model"],"metadata":{"execution":{"iopub.status.busy":"2021-05-30T16:56:32.643879Z","iopub.execute_input":"2021-05-30T16:56:32.64429Z","iopub.status.idle":"2021-05-30T16:56:32.649002Z","shell.execute_reply.started":"2021-05-30T16:56:32.644255Z","shell.execute_reply":"2021-05-30T16:56:32.648019Z"},"id":"J8BCe7C9Swcw"}},{"cell_type":"code","source":["model.save('model_v1_inceptionV3.h5')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:02:06.294897Z","iopub.execute_input":"2021-06-01T17:02:06.29516Z","iopub.status.idle":"2021-06-01T17:03:28.824529Z","shell.execute_reply.started":"2021-06-01T17:02:06.295134Z","shell.execute_reply":"2021-06-01T17:03:28.823451Z"},"trusted":true,"id":"rHU6wPJ_Swcw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Accuracy and Loss curves"],"metadata":{"id":"ZkCWeZzHSwcw"}},{"cell_type":"code","source":["def plot_accuracy(history):\n","\n","    plt.plot(history.history['accuracy'],label='train accuracy')\n","    plt.plot(history.history['val_accuracy'],label='validation accuracy')\n","    plt.title('Model accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='best')\n","    plt.savefig('Accuracy_v1_InceptionV3')\n","    plt.show()\n","\n","def plot_loss(history):\n","\n","    plt.plot(history.history['loss'],label=\"train loss\")\n","    plt.plot(history.history['val_loss'],label=\"validation loss\")\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loc='best')\n","    plt.savefig('Loss_v1_InceptionV3')\n","    plt.show()\n","\n","plot_accuracy(history)\n","plot_loss(history)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:03:28.832398Z","iopub.execute_input":"2021-06-01T17:03:28.834773Z","iopub.status.idle":"2021-06-01T17:03:29.320794Z","shell.execute_reply.started":"2021-06-01T17:03:28.83471Z","shell.execute_reply":"2021-06-01T17:03:29.320022Z"},"trusted":true,"id":"qE5slMAzSwcw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <b> PREDICTIONS </b>"],"metadata":{"id":"A3pal1pwSwcx"}},{"cell_type":"markdown","source":["## Load the model"],"metadata":{"id":"j7S68VIHSwcx"}},{"cell_type":"code","source":["K.clear_session()\n","path_to_model='./model_v1_inceptionV3.h5'\n","print(\"Loading the model..\")\n","model = load_model(path_to_model)\n","print(\"Done!\")"],"metadata":{"execution":{"iopub.status.busy":"2021-06-02T12:23:01.382695Z","iopub.execute_input":"2021-06-02T12:23:01.383058Z","iopub.status.idle":"2021-06-02T12:23:01.387009Z","shell.execute_reply.started":"2021-06-02T12:23:01.383028Z","shell.execute_reply":"2021-06-02T12:23:01.386065Z"},"trusted":true,"id":"XqXjRzoaSwcx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing model on test set"],"metadata":{"id":"CZBTbcufSwcx"}},{"cell_type":"code","source":["test_data_dir = '../input/indian-food-classification/test'\n","\n","test_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","test_generator = test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:03:42.751944Z","iopub.execute_input":"2021-06-01T17:03:42.752289Z","iopub.status.idle":"2021-06-01T17:03:42.872027Z","shell.execute_reply.started":"2021-06-01T17:03:42.752253Z","shell.execute_reply":"2021-06-01T17:03:42.87124Z"},"trusted":true,"id":"RgQD20HaSwcx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores = model.evaluate_generator(test_generator)\n","\n","print(\"Test Accuracy: {:.3f}\".format(scores[1]))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:03:42.873299Z","iopub.execute_input":"2021-06-01T17:03:42.873642Z","iopub.status.idle":"2021-06-01T17:03:47.862124Z","shell.execute_reply.started":"2021-06-01T17:03:42.873607Z","shell.execute_reply":"2021-06-01T17:03:47.860705Z"},"trusted":true,"id":"0-OsJ_boSwcx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Function to predict single image or predict all images from a directory"],"metadata":{"id":"Syi1wVEHSwcy"}},{"cell_type":"code","source":["category={\n","    0: ['burger','Burger'], 1: ['butter_naan','Butter Naan'], 2: ['chai','Chai'],\n","    3: ['chapati','Chapati'], 4: ['chole_bhature','Chole Bhature'], 5: ['dal_makhani','Dal Makhani'],\n","    6: ['dhokla','Dhokla'], 7: ['fried_rice','Fried Rice'], 8: ['idli','Idli'], 9: ['jalegi','Jalebi'],\n","    10: ['kathi_rolls','Kaathi Rolls'], 11: ['kadai_paneer','Kadai Paneer'], 12: ['kulfi','Kulfi'],\n","    13: ['masala_dosa','Masala Dosa'], 14: ['momos','Momos'], 15: ['paani_puri','Paani Puri'],\n","    16: ['pakode','Pakode'], 17: ['pav_bhaji','Pav Bhaji'], 18: ['pizza','Pizza'], 19: ['samosa','Samosa']\n","}\n","\n","def predict_image(filename,model):\n","    img_ = image.load_img(filename, target_size=(299, 299))\n","    img_array = image.img_to_array(img_)\n","    img_processed = np.expand_dims(img_array, axis=0)\n","    img_processed /= 255.\n","\n","    prediction = model.predict(img_processed)\n","\n","    index = np.argmax(prediction)\n","\n","    plt.title(\"Prediction - {}\".format(category[index][1]))\n","    plt.imshow(img_array)\n","\n","def predict_dir(filedir,model):\n","    cols=5\n","    pos=0\n","    images=[]\n","    total_images=len(os.listdir(filedir))\n","    rows=total_images//cols + 1\n","\n","    true=filedir.split('/')[-1]\n","\n","    fig=plt.figure(1, figsize=(25, 25))\n","\n","    for i in sorted(os.listdir(filedir)):\n","        images.append(os.path.join(filedir,i))\n","\n","    for subplot,imggg in enumerate(images):\n","        img_ = image.load_img(imggg, target_size=(299, 299))\n","        img_array = image.img_to_array(img_)\n","\n","        img_processed = np.expand_dims(img_array, axis=0)\n","\n","        img_processed /= 255.\n","        prediction = model.predict(img_processed)\n","        index = np.argmax(prediction)\n","\n","        pred=category.get(index)[0]\n","        if pred==true:\n","            pos+=1\n","\n","        fig=plt.subplot(rows, cols, subplot+1)\n","        fig.set_title(category.get(index)[1], pad = 10,size=18)\n","        plt.imshow(img_array)\n","\n","    acc=pos/total_images\n","    print(\"Accuracy of Test : {:.2f} ({pos}/{total})\".format(acc,pos=pos,total=total_images))\n","    plt.tight_layout()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:31:56.994537Z","iopub.execute_input":"2021-06-01T17:31:56.994847Z","iopub.status.idle":"2021-06-01T17:31:57.009481Z","shell.execute_reply.started":"2021-06-01T17:31:56.994818Z","shell.execute_reply":"2021-06-01T17:31:57.008643Z"},"trusted":true,"id":"fCVicsfySwcy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- <h3> Single image prediction </h3>"],"metadata":{"id":"UEshxMLoSwcy"}},{"cell_type":"code","source":["predict_image('../input/indian-food-classification/test/burger/images (16).jpg',model)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:31:59.752706Z","iopub.execute_input":"2021-06-01T17:31:59.753038Z","iopub.status.idle":"2021-06-01T17:31:59.960626Z","shell.execute_reply.started":"2021-06-01T17:31:59.753009Z","shell.execute_reply":"2021-06-01T17:31:59.959859Z"},"trusted":true,"id":"khg4XBV5Swcy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- <h3> Predicting category </h3>"],"metadata":{"id":"K_JCcAGQSwcy"}},{"cell_type":"code","source":["predict_dir(\"../input/indian-food-classification/test/masala_dosa\",model)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:31:25.846313Z","iopub.execute_input":"2021-06-01T17:31:25.846622Z","iopub.status.idle":"2021-06-01T17:31:32.304295Z","shell.execute_reply.started":"2021-06-01T17:31:25.846594Z","shell.execute_reply":"2021-06-01T17:31:32.303317Z"},"trusted":true,"id":"ZWKOMEdySwcz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Let's plot a confusion matrix for all the food items"],"metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:20:10.133029Z","iopub.execute_input":"2021-05-31T18:20:10.133343Z","iopub.status.idle":"2021-05-31T18:20:10.136472Z","shell.execute_reply.started":"2021-05-31T18:20:10.133318Z","shell.execute_reply":"2021-05-31T18:20:10.13557Z"},"id":"AnU3OAJXSwcz"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","img_width, img_height = 299, 299\n","\n","def labels_confusion_matrix():\n","    folder_path=\"../input/indian-food-classification/test\"\n","\n","    mapping={}\n","    for i,j in enumerate(sorted(os.listdir(folder_path))):\n","        mapping[j]=i\n","\n","    files=[]\n","    real=[]\n","    predicted=[]\n","\n","    for i in os.listdir(folder_path):\n","\n","        true=os.path.join(folder_path,i)\n","        true=true.split('/')[-1]\n","        true=mapping[true]\n","\n","        for j in os.listdir(os.path.join(folder_path,i)):\n","\n","            img_ = image.load_img(os.path.join(folder_path,i,j), target_size=(img_height, img_width))\n","            img_array = image.img_to_array(img_)\n","            img_processed = np.expand_dims(img_array, axis=0)\n","            img_processed /= 255.\n","            prediction = model.predict(img_processed)\n","            index = np.argmax(prediction)\n","\n","            predicted.append(index)\n","            real.append(true)\n","\n","    return (real,predicted)\n","\n","def print_confusion_matrix(real,predicted):\n","\n","    cmap=\"viridis\"\n","    cm_plot_labels = [i for i in range(20)]\n","\n","    cm = confusion_matrix(y_true=real, y_pred=predicted)\n","    df_cm = pd.DataFrame(cm,cm_plot_labels,cm_plot_labels)\n","    sns.set(font_scale=1.1) # for label size\n","    plt.figure(figsize = (15,10))\n","    s=sns.heatmap(df_cm, annot=True,cmap=cmap) # font size\n","#     bottom,top=s.get_ylim()\n","#     s.set_ylim(bottom+0.6,top-0.6)\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.savefig('confusion_matrix.png')\n","    plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:33:53.022667Z","iopub.execute_input":"2021-06-01T17:33:53.022995Z","iopub.status.idle":"2021-06-01T17:33:53.033624Z","shell.execute_reply.started":"2021-06-01T17:33:53.022965Z","shell.execute_reply":"2021-06-01T17:33:53.032803Z"},"trusted":true,"id":"hLF0jMrLSwcz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true,y_pred=labels_confusion_matrix()\n","print_confusion_matrix(y_true,y_pred)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:33:53.972331Z","iopub.execute_input":"2021-06-01T17:33:53.972641Z","iopub.status.idle":"2021-06-01T17:34:21.962452Z","shell.execute_reply.started":"2021-06-01T17:33:53.972611Z","shell.execute_reply":"2021-06-01T17:34:21.961662Z"},"trusted":true,"id":"epl8sOSSSwcz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<span style=\"font-size:21px;color:blue\"><b>So far, So good!</b></span>\n","<hr>"],"metadata":{"id":"D8mhqviHSwc0"}},{"cell_type":"markdown","source":["<h1> <b> MODEL LEARNING VISUALIZATIONS </b></h1>"],"metadata":{"id":"TU9Ar51wSwc0"}},{"cell_type":"markdown","source":["### SOME HELPER FUNCTIONS WHICH WILL ENABLE US TO VISUALIZE HOW NEURAL NETWORK WORKS AND PERFORMS!"],"metadata":{"id":"abMmaXBnSwc0"}},{"cell_type":"code","source":["def get_activations(img, model_activations):\n","    img = image.load_img(img, target_size=(299, 299))\n","    img = image.img_to_array(img)\n","    img = np.expand_dims(img, axis=0)\n","    img /= 255.\n","    plt.imshow(img[0])\n","    plt.show()\n","    return model_activations.predict(img)\n","\n","def show_activations(activations, layer_names):\n","\n","    images_per_row = 16\n","\n","    # Now let's display our feature maps\n","    for layer_name, layer_activation in zip(layer_names, activations):\n","        # This is the number of features in the feature map\n","        n_features = layer_activation.shape[-1]\n","\n","        # The feature map has shape (1, size, size, n_features)\n","        size = layer_activation.shape[1]\n","\n","        # We will tile the activation channels in this matrix\n","        n_cols = n_features // images_per_row\n","        display_grid = np.zeros((size * n_cols, images_per_row * size))\n","\n","        # We'll tile each filter into this big horizontal grid\n","        for col in range(n_cols):\n","            for row in range(images_per_row):\n","                channel_image = layer_activation[0,:, :,col * images_per_row + row]\n","                # Post-process the feature to make it visually palatable\n","                channel_image -= channel_image.mean()\n","                channel_image /= channel_image.std()\n","                channel_image *= 64\n","                channel_image += 128\n","                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n","                display_grid[col * size : (col + 1) * size,row * size : (row + 1) * size] = channel_image\n","\n","        # Display the grid\n","        scale = 1. / size\n","        plt.figure(figsize=(scale * display_grid.shape[1],\n","                            scale * display_grid.shape[0]))\n","        plt.title(layer_name)\n","        plt.grid(False)\n","        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n","\n","    plt.show()\n","\n","def activation_conv():\n","    first_convlayer_activation = activations[0]\n","    second_convlayer_activation = activations[3]\n","    third_convlayer_activation = activations[6]\n","    f,ax = plt.subplots(1,3, figsize=(10,10))\n","    ax[0].imshow(first_convlayer_activation[0, :, :, 3], cmap='viridis')\n","    ax[0].axis('OFF')\n","    ax[0].set_title('Conv2d_1')\n","    ax[1].imshow(second_convlayer_activation[0, :, :, 3], cmap='viridis')\n","    ax[1].axis('OFF')\n","    ax[1].set_title('Conv2d_2')\n","    ax[2].imshow(third_convlayer_activation[0, :, :, 3], cmap='viridis')\n","    ax[2].axis('OFF')\n","    ax[2].set_title('Conv2d_3')\n","\n","\n","def get_attribution(food):\n","\n","    tf.compat.v1.disable_eager_execution()\n","\n","    img = image.load_img(food, target_size=(299, 299))\n","    img = image.img_to_array(img)\n","    img /= 255.\n","    f,ax = plt.subplots(1,3, figsize=(15,15))\n","    ax[0].imshow(img)\n","\n","    img = np.expand_dims(img, axis=0)\n","    model = load_model('./model_v1_inceptionV3.h5')\n","\n","    preds = model.predict(img)\n","    class_id = np.argmax(preds[0])\n","    ax[0].set_title(\"Input Image\")\n","    class_output = model.output[:, class_id]\n","    last_conv_layer = model.get_layer(\"mixed10\")\n","\n","    grads = K.gradients(class_output, last_conv_layer.output)[0]\n","    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n","    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n","    pooled_grads_value, conv_layer_output_value = iterate([img])\n","    for i in range(2048):\n","        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n","\n","    heatmap = np.mean(conv_layer_output_value, axis=-1)\n","    heatmap = np.maximum(heatmap, 0)\n","    heatmap /= np.max(heatmap)\n","    ax[1].imshow(heatmap)\n","    ax[1].set_title(\"Heat map\")\n","\n","\n","    act_img = cv2.imread(food)\n","    heatmap = cv2.resize(heatmap, (act_img.shape[1], act_img.shape[0]))\n","    heatmap = np.uint8(255 * heatmap)\n","    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n","    superimposed = cv2.addWeighted(act_img, 0.6, heatmap, 0.4, 0)\n","    cv2.imwrite('classactivation.png', superimposed)\n","    img_act = image.load_img('classactivation.png', target_size=(299, 299))\n","    ax[2].imshow(img_act)\n","    ax[2].set_title(\"Class Activation\")\n","    plt.show()\n","    return preds"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:04:12.614603Z","iopub.execute_input":"2021-06-01T17:04:12.615046Z","iopub.status.idle":"2021-06-01T17:04:12.63972Z","shell.execute_reply.started":"2021-06-01T17:04:12.615002Z","shell.execute_reply":"2021-06-01T17:04:12.63891Z"},"trusted":true,"id":"q0G8UTrySwc0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## MODEL LAYERS"],"metadata":{"id":"g3WLObJsSwc1"}},{"cell_type":"code","source":["print(\"Total layers in the model : \",len(model.layers),\"\\n\")\n","\n","# We start with index 1 instead of 0, as input layer is at index 0\n","layers = [layer.output for layer in model.layers[1:11]]\n","# We now initialize a model which takes an input and outputs the above chosen layers\n","activations_output = models.Model(inputs=model.input, outputs=layers)\n","# print(layers)\n","\n","layer_names = []\n","for layer in model.layers[1:11]:\n","    layer_names.append(layer.name)\n","\n","print(\"First 10 layers which we can visualize are -> \", layer_names)\n"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:04:12.641015Z","iopub.execute_input":"2021-06-01T17:04:12.64138Z","iopub.status.idle":"2021-06-01T17:04:12.660204Z","shell.execute_reply.started":"2021-06-01T17:04:12.641344Z","shell.execute_reply":"2021-06-01T17:04:12.659175Z"},"trusted":true,"id":"HJrbEuFRSwc1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<span style=\"font-size:18px;color:blue\"><b>Our model has 315 layers with InceptionV3 architecture!</b></span>\n","<br><br>\n","<span style=\"font-size:18px;color:blue\"><b>Whoa! That's a lot to process at first, so let's just stick with 10 layers for now and visualize these first to see how neural networks classify.</b></span>\n","<hr>"],"metadata":{"id":"lDiGcFXuSwc1"}},{"cell_type":"markdown","source":["# <b>LAYER WISE ACTIVATIONS</b>"],"metadata":{"id":"6iKwDxFwSwc1"}},{"cell_type":"code","source":["food = '../input/indian-food-classification/dataset/Dataset/val/pizza/155.jpg'\n","activations = get_activations(food,activations_output)\n","show_activations(activations, layer_names)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:04:12.661483Z","iopub.execute_input":"2021-06-01T17:04:12.661853Z","iopub.status.idle":"2021-06-01T17:04:17.316974Z","shell.execute_reply.started":"2021-06-01T17:04:12.661815Z","shell.execute_reply":"2021-06-01T17:04:17.315947Z"},"trusted":true,"id":"ia7rYu56Swc1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Let's show the activation outputs of Conv2D layer (we have three of them in first 10 layers) to compare how layers get abstract with depth."],"metadata":{"id":"QFFKzQE-Swc2"}},{"cell_type":"code","source":["activation_conv()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:04:17.318514Z","iopub.execute_input":"2021-06-01T17:04:17.318855Z","iopub.status.idle":"2021-06-01T17:04:17.658724Z","shell.execute_reply.started":"2021-06-01T17:04:17.318822Z","shell.execute_reply":"2021-06-01T17:04:17.657779Z"},"trusted":true,"id":"adc3v5AJSwc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## This time let's visualize some other food item's layer."],"metadata":{"id":"-o1NQTE4Swc2"}},{"cell_type":"code","source":["food = '../input/indian-food-classification/dataset/Dataset/val/idli/065.jpg'\n","activations = get_activations(food,activations_output)\n","show_activations(activations, layer_names)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:04:17.660214Z","iopub.execute_input":"2021-06-01T17:04:17.66055Z","iopub.status.idle":"2021-06-01T17:04:20.914601Z","shell.execute_reply.started":"2021-06-01T17:04:17.660512Z","shell.execute_reply":"2021-06-01T17:04:20.913662Z"},"trusted":true,"id":"giDwEavKSwc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activation_conv()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:04:20.916025Z","iopub.execute_input":"2021-06-01T17:04:20.916402Z","iopub.status.idle":"2021-06-01T17:04:21.152667Z","shell.execute_reply.started":"2021-06-01T17:04:20.916363Z","shell.execute_reply":"2021-06-01T17:04:21.151748Z"},"trusted":true,"id":"k3-rWKv7Swc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2><span style=\"color:blue\">NOW THIS IS WHERE THINGS WILL GET INTERESTING!</span></h2>\n","\n","- <span style=\"font-size:17px;\"> So far we were doing activation maps visualization which helped us understand how the input is transformed from one layer to another as it goes through several operations. </span>\n","\n","- <span style=\"font-size:17px;\"> At the end of training, we want the model to classify or detect objects based on features which are specific to the class.</span>\n","\n","- <span style=\"font-size:17px;\"> To validate how model attributes the features to class output, we can generate heat maps using gradients to find out which regions in the input images were instrumental in determining the class. </span>"],"metadata":{"id":"CTSIBTRISwc3"}},{"cell_type":"markdown","source":["# <b>GENERATING HEATMAPS</b>"],"metadata":{"id":"1zHBmBgVSwc3"}},{"cell_type":"code","source":["pred = get_attribution('../input/indian-food-classification/dataset/Dataset/val/idli/065.jpg')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:04:21.15405Z","iopub.execute_input":"2021-06-01T17:04:21.154379Z","iopub.status.idle":"2021-06-01T17:04:48.076377Z","shell.execute_reply.started":"2021-06-01T17:04:21.154347Z","shell.execute_reply":"2021-06-01T17:04:48.075586Z"},"trusted":true,"id":"StS-8MnQSwc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred2=get_attribution('../input/indian-food-classification/test/fried_rice/images (5).jpg')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:04:48.078562Z","iopub.execute_input":"2021-06-01T17:04:48.079017Z","iopub.status.idle":"2021-06-01T17:05:20.149136Z","shell.execute_reply.started":"2021-06-01T17:04:48.078979Z","shell.execute_reply":"2021-06-01T17:05:20.148328Z"},"trusted":true,"id":"Z32xKcrbSwc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred3=get_attribution('../input/indian-food-classification/test/chai/images (3).jpg')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:05:20.151617Z","iopub.execute_input":"2021-06-01T17:05:20.151986Z","iopub.status.idle":"2021-06-01T17:06:01.146133Z","shell.execute_reply.started":"2021-06-01T17:05:20.151948Z","shell.execute_reply":"2021-06-01T17:06:01.145314Z"},"trusted":true,"id":"b6ZaDic6Swc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred4=get_attribution('../input/indian-food-classification/test/jalebi/images (4).jpg')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:06:01.148281Z","iopub.execute_input":"2021-06-01T17:06:01.148757Z","iopub.status.idle":"2021-06-01T17:06:47.713062Z","shell.execute_reply.started":"2021-06-01T17:06:01.148716Z","shell.execute_reply":"2021-06-01T17:06:47.712265Z"},"trusted":true,"id":"2ETJz1-ASwc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred5=get_attribution('../input/indian-food-classification/test/chole_bhature/images (10).jpg')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:06:47.71602Z","iopub.execute_input":"2021-06-01T17:06:47.716306Z","iopub.status.idle":"2021-06-01T17:07:42.192712Z","shell.execute_reply.started":"2021-06-01T17:06:47.716279Z","shell.execute_reply":"2021-06-01T17:07:42.191902Z"},"trusted":true,"id":"_XCqw7VZSwc3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2><span style=\"color:green\">WOW! WE DID IT!</span></h2>\n","<hr></hr>\n","<ol>\n","    <h3>\n","        <li>In the above plot, we see on left the input image passed to the model, heat map in the middle and the class activation map on right</li><br>\n","        <li>Heat map gives a visual of what regions in the image were used in determining the class of the image</li><br>\n","        <li>Now it's clearly visible what a model looks for in an image if it has to be classified as an idli!</li>\n","    </h3>\n","</ol>"],"metadata":{"id":"2p71qPOnSwc4"}},{"cell_type":"markdown","source":["## Downloading random image from net to predict and generate heatmap"],"metadata":{"execution":{"iopub.status.busy":"2021-05-30T14:23:15.389003Z","iopub.execute_input":"2021-05-30T14:23:15.389366Z","iopub.status.idle":"2021-05-30T14:23:15.39501Z","shell.execute_reply.started":"2021-05-30T14:23:15.389333Z","shell.execute_reply":"2021-05-30T14:23:15.393067Z"},"id":"o3yA8btoSwc4"}},{"cell_type":"code","source":["!wget -O download.jpg https://www.cookwithmanali.com/wp-content/uploads/2015/01/Restaurant-Style-Dal-Makhani-Recipe.jpg\n","\n","model_load = load_model('./model_v1_inceptionV3.h5')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:07:42.195149Z","iopub.execute_input":"2021-06-01T17:07:42.195645Z","iopub.status.idle":"2021-06-01T17:08:37.219755Z","shell.execute_reply.started":"2021-06-01T17:07:42.195599Z","shell.execute_reply":"2021-06-01T17:08:37.21888Z"},"trusted":true,"id":"zIaDLjsxSwc4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<span style=\"font-size:16px;\"><b>This image was given to confuse the model as both classes <u>Butter naan</u> and <u>Dal makhni</u> are present. But it predicted dal makhni and why was that?<br><br>Because the output layer inside the model computed high activations for dal makhni object. It does happen sometimes due to centralized nature of model's focus area. </b></span>"],"metadata":{"id":"lKJQo3aTSwc4"}},{"cell_type":"code","source":["pred = get_attribution('download.jpg')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:08:37.221237Z","iopub.execute_input":"2021-06-01T17:08:37.221599Z","iopub.status.idle":"2021-06-01T17:09:58.811774Z","shell.execute_reply.started":"2021-06-01T17:08:37.221557Z","shell.execute_reply":"2021-06-01T17:09:58.810959Z"},"trusted":true,"id":"BxkE8JEOSwc4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<hr>\n","<h2><div style=\"color:purple;text-align: center;font-weight:bold;\">THANK YOU FOR BEARING WITH ME!</div></h2>\n","<hr>\n","<h2><center>If you liked this kernel or the yummy dataset, please consider upvoting.<br>Happy Kaggling.</center></h2>\n","<hr>\n"],"metadata":{"id":"c81FDrg1Swc5"}}]}